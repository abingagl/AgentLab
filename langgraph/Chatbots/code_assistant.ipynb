{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U langchain_community langchain-openai langchain-anthropic langchain langgraph bs4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"CodeAssistant\"\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "# _set_env(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "# LCEL docs\n",
    "url = \"https://python.langchain.com/v0.2/docs/concepts/#langchain-expression-language-lcel\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Sort the list based on the URLs and get the text\n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "### OpenAI\n",
    "\n",
    "# Grader prompt\n",
    "code_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a coding assistant with expertise in LCEL, LangChain expression language. \\n \n",
    "    Here is a full set of LCEL documentation:  \\n ------- \\n  {context} \\n ------- \\n Answer the user \n",
    "    question based on the above provided documentation. Ensure any code you provide can be executed \\n \n",
    "    with all required imports and variables defined. Structure your answer with a description of the code solution. \\n\n",
    "    Then list the imports. And finally list the functioning code block. Here is the user question:\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Data model\n",
    "class code(BaseModel):\n",
    "    \"\"\"Code output\"\"\"\n",
    "\n",
    "    prefix: str = Field(description=\"Description of the problem and approach\")\n",
    "    imports: str = Field(description=\"Code block import statements\")\n",
    "    code: str = Field(description=\"Code block not including import statements\")\n",
    "    description = \"Schema for code solutions to questions about LCEL.\"\n",
    "\n",
    "\n",
    "expt_llm = \"gpt-4o-mini\"\n",
    "llm = ChatOpenAI(temperature=0, model=expt_llm)\n",
    "code_gen_chain = code_gen_prompt | llm.with_structured_output(code)\n",
    "question = \"How do I build a RAG chain in LCEL?\"\n",
    "# solution = code_gen_chain_oai.invoke({\"context\":concatenated_content,\"messages\":[(\"user\",question)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test\n",
    "# question = \"How do I build a RAG chain in LCEL?\"\n",
    "# solution = code_gen_chain.invoke(\n",
    "#     {\"context\": concatenated_content, \"messages\": [(\"user\", question)]}\n",
    "# )\n",
    "# solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        error : Binary flag for control flow to indicate whether test error was tripped\n",
    "        messages : With user question, error messages, reasoning\n",
    "        generation : Code solution\n",
    "        iterations : Number of tries\n",
    "    \"\"\"\n",
    "\n",
    "    error: str\n",
    "    messages: List\n",
    "    generation: str\n",
    "    iterations: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "### Parameter\n",
    "\n",
    "# Max tries\n",
    "max_iterations = 3\n",
    "# Reflect\n",
    "# flag = 'reflect'\n",
    "flag = \"do not reflect\"\n",
    "\n",
    "### Nodes\n",
    "\n",
    "\n",
    "def generate(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generate a code solution\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GENERATING CODE SOLUTION---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "    error = state.get(\"error\", \"no\")\n",
    "\n",
    "\n",
    "    # We have been routed back to generation with an error\n",
    "    if error == \"yes\":\n",
    "        messages += [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:\",\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    # Solution\n",
    "    code_solution = code_gen_chain.invoke(\n",
    "        {\"context\": concatenated_content, \"messages\": messages}\n",
    "    )\n",
    "    messages += [\n",
    "        (\n",
    "            \"assistant\",\n",
    "            f\"{code_solution.prefix} \\n Imports: {code_solution.imports} \\n Code: {code_solution.code}\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Increment\n",
    "    iterations = iterations + 1\n",
    "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
    "\n",
    "\n",
    "def code_check(state: GraphState):\n",
    "    \"\"\"\n",
    "    Check code\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, error\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECKING CODE---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    code_solution = state[\"generation\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "\n",
    "    # Get solution components\n",
    "    imports = code_solution.imports\n",
    "    code = code_solution.code\n",
    "\n",
    "    # Check imports\n",
    "    try:\n",
    "        exec(imports)\n",
    "    except Exception as e:\n",
    "        print(\"---CODE IMPORT CHECK: FAILED---\")\n",
    "        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n",
    "        messages += error_message\n",
    "        return {\n",
    "            \"generation\": code_solution,\n",
    "            \"messages\": messages,\n",
    "            \"iterations\": iterations,\n",
    "            \"error\": \"yes\",\n",
    "        }\n",
    "\n",
    "    # Check execution\n",
    "    try:\n",
    "        exec(imports + \"\\n\" + code)\n",
    "    except Exception as e:\n",
    "        print(\"---CODE BLOCK CHECK: FAILED---\")\n",
    "        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n",
    "        messages += error_message\n",
    "        return {\n",
    "            \"generation\": code_solution,\n",
    "            \"messages\": messages,\n",
    "            \"iterations\": iterations,\n",
    "            \"error\": \"yes\",\n",
    "        }\n",
    "\n",
    "    # No errors\n",
    "    print(\"---NO CODE TEST FAILURES---\")\n",
    "    return {\n",
    "        \"generation\": code_solution,\n",
    "        \"messages\": messages,\n",
    "        \"iterations\": iterations,\n",
    "        \"error\": \"no\",\n",
    "    }\n",
    "\n",
    "\n",
    "def reflect(state: GraphState):\n",
    "    \"\"\"\n",
    "    Reflect on errors\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GENERATING CODE SOLUTION---\")\n",
    "\n",
    "    # State\n",
    "    messages = state[\"messages\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "    code_solution = state[\"generation\"]\n",
    "\n",
    "    # Prompt reflection\n",
    "\n",
    "    # Add reflection\n",
    "    reflections = code_gen_chain.invoke(\n",
    "        {\"context\": concatenated_content, \"messages\": messages}\n",
    "    )\n",
    "    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n",
    "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
    "\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def decide_to_finish(state: GraphState):\n",
    "    \"\"\"\n",
    "    Determines whether to finish.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "    error = state[\"error\"]\n",
    "    iterations = state[\"iterations\"]\n",
    "\n",
    "    if error == \"no\" or iterations == max_iterations:\n",
    "        print(\"---DECISION: FINISH---\")\n",
    "        return \"end\"\n",
    "    else:\n",
    "        print(\"---DECISION: RE-TRY SOLUTION---\")\n",
    "        if flag == \"reflect\":\n",
    "            return \"reflect\"\n",
    "        else:\n",
    "            return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"generate\", generate)  # generation solution\n",
    "workflow.add_node(\"check_code\", code_check)  # check code\n",
    "workflow.add_node(\"reflect\", reflect)  # reflect\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"generate\")\n",
    "workflow.add_edge(\"generate\", \"check_code\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_code\",\n",
    "    decide_to_finish,\n",
    "    {\n",
    "        \"end\": END,\n",
    "        \"reflect\": \"reflect\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"reflect\", \"generate\")\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/4gHYSUNDX1BST0ZJTEUAAQEAAAHIAAAAAAQwAABtbnRyUkdCIFhZWiAH4AABAAEAAAAAAABhY3NwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAA9tYAAQAAAADTLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAlkZXNjAAAA8AAAACRyWFlaAAABFAAAABRnWFlaAAABKAAAABRiWFlaAAABPAAAABR3dHB0AAABUAAAABRyVFJDAAABZAAAAChnVFJDAAABZAAAAChiVFJDAAABZAAAAChjcHJ0AAABjAAAADxtbHVjAAAAAAAAAAEAAAAMZW5VUwAAAAgAAAAcAHMAUgBHAEJYWVogAAAAAAAAb6IAADj1AAADkFhZWiAAAAAAAABimQAAt4UAABjaWFlaIAAAAAAAACSgAAAPhAAAts9YWVogAAAAAAAA9tYAAQAAAADTLXBhcmEAAAAAAAQAAAACZmYAAPKnAAANWQAAE9AAAApbAAAAAAAAAABtbHVjAAAAAAAAAAEAAAAMZW5VUwAAACAAAAAcAEcAbwBvAGcAbABlACAASQBuAGMALgAgADIAMAAxADb/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wAARCAF0AOQDASIAAhEBAxEB/8QAHQABAAMAAwEBAQAAAAAAAAAAAAUGBwMECAECCf/EAF4QAAEEAQIDAgYKDQcIBQ0AAAEAAgMEBQYRBxIhEzEIFBUiQZQWF1FVVmF10dLTIzIzNTdUcZGTlaGztDQ2QlNystQkQ1Jkc4GxtWJjg4TwCRglREVGV3SCoqOkw//EABoBAQEAAwEBAAAAAAAAAAAAAAABAgMEBQb/xAA2EQEAAQIBCQQJBQADAAAAAAAAAQIRAwQSFCExUVKR0UFhcaEFEyMyM2KBscEVY5Ki8CLh8f/aAAwDAQACEQMRAD8A/qmiIgIiICIiAiIgIiICIonOZqSg+CnSh8bylrfsYidmMA75JD6GN3G/pJIA6lZU0zVNoEq5wY0ucQ1oG5JOwAUa/U2HjcWvy1Frh6DZYD/xUWzQNC85s+ec7UVrfm/y8B0DD/1cH2jQPQdi7u3cSN1JM0ng42BrcNj2tHQAVWAD9i3WwY2zM+Ef77Qup99lWF9+KHrLPnT2VYX34oess+dPYrhfeeh6sz5k9iuF956HqzPmT2Pf5LqPZVhffih6yz509lWF9+KHrLPnT2K4X3noerM+ZPYrhfeeh6sz5k9j3+RqPZVhffih6yz509lWF9+KHrLPnT2K4X3noerM+ZPYrhfeeh6sz5k9j3+Rqc1bO427IGV8hVneegbFO1x/MCu8oaxovT9thZPgsbK0gjZ9SM9/Q+hdH2P29MfZ8FJLPVbsZMPYlL2OaO/sHuO8b/cBPIdtiG784ZuHVqpm09/X/eKalnRdXF5KvmMfBdqvL4Jm8zeZpa4e61zT1a4HcFp2IIIOxC7S0TExNpQREUBERAREQEREBERAREQEREBERAVY0htlMjnc0/Zz5bklGE9d2Q13OjLf0omd/wDUPcVnVZ0G3xSnlse4ES1Mrb5gRt0lldYZt7vmTN6/EV0UasOuY26uX/tljYsyIi50dDPZ3H6Xwt7L5a5FQxlGF9izandysijaN3OJ+IBZDrrwrdKaf4VZHWmCFvPRVbtWj4u7H267ueZ7QHODoeYNDHF4cW8riGtB3e3fRuJ+OxuX4d6jpZjDW9Q4uejLHZxVBnPYtMLTvHGN2+efRsR126heZbuN1/rHgVxKwMON1PmsHjrGLn0z7JaHiuYtRQzRT2YHMIa6Tk7LZj3NDnkkbu2BQb5nfCB0PpnA4fMZTI36VPLCU02SYa74w8RODZHOg7HtWBpI3L2gbEHuIK581x40HgMRpzKXNRQ+Iaja52JnrQy2Bc5Wc5azs2OPMR0DT1J80Au6LLeI2ts5rLUOlbhxPETF8Pp6Vo2KuAxtmpk5Mg2SMRMsBgE0MJYZCHAtaXfbO2AVQ4NaE1Bjq/g908npnL0pNO5fUTcgy9Ve7xPnjtdi58mxaWu52BsgcWuJGxJQa3g/CbwOe4wv0RDRyjI34yndrXpMReaZJLBeQx7XQDsWhjWHtHkN3c5vQscFsiw/J2MhonworOYsaezWRwuotP0MXXyOKovtQ17EVqcvbOWA9k3lnY7nds3YO67jZbggIiIKviNsTrfL45mza12CPJRsH9GUuMc35AdondPS557zubQqxCPHOJNmRu5ZQxjIXO26c8shdtv7oETSf7Q91WddGNtie20fbosiIi50EREBERAREQEREBERAREQEREBV7L0rGJyxztCA2eeJsN6qz7eWNpcWvYPS9vM7p/Sadu8NCsKLOiuaJVW8tg9KcVdPsr5ShjdT4cyiQQXIWTxNkbuOrXA7PbuRsRuOo6KtjwbOFABA4b6WAPQ7YmDr/8AarXlNF4vK3HXTHNTvuABuUJ315Xbd3MWEc4+J24XUOiJwAG6nzzGj0dvEf2mMlbc3Cq2VW8Y6f8ARqdHTfBHh9o7MwZbBaJwGHykHMIrlHHRRSx8zS12zmtBG7SQfiJV2VX9hNj4VZ79ND9UnsJsfCrPfpofqk9Xh8flJaN60Isr4o4/K6P0ZPlMfqnMG0y1ThAnlhLeWW1FE/8AzY68r3bfHsrZ7CbHwqz36aH6pPV4fH5SWjess0LLEL4pWNkie0tcxw3DgehBCzn/AM2rhP8A/DbS36og+irD7CbHwqz36aH6pPYTY+FWe/TQ/VJ6vD4/KS0b1eHg1cJgPwbaW/VEH0VcsxqGHFSR04GeOZSYfYKMR84ju53kA8kY9LyNvQN3ENMcNDGTpY1FnbDOoLPHBFuPyxta4flB3UvhtP47T8MkePqR1u0IdK8bukldtsHPed3PO3TdxJS2FTrvfyj/AH+uanHp3CnDU5e2kbPftSmzcnaCBJKQASASSGhrWtaCTs1jRudt1Koi01VTVN5QREWIIiICIiAiIgIiICIiAiIgIiICIiAiIgIiIM949EDhnb5iQPH8b3f/AD9f4wtCWe8et/azt7bfy/G/bAfj9f3VoSAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiICIiAiIgIiIM84+Dfhlb3cG/8ApDG9SP8AX660NZ5x829rK3v0HlDG9w3/APX660NAREQEREBERAREQEREBERAREQEREBERAREQEREBF+ZJGxMc97gxjQS5zjsAPdKpZ1fncs0WcPi6QxzwHQzZCy+OSZvof2bYzyg943O+x6hp6Ldh4VWLfNW112RUjy7rD8Qwfrc31aeXdYfiGD9bm+rW7Ra98c4LLuipHl3WH4hg/W5vq08u6w/EMH63N9Wmi1745wWYB4dHhN2eCsWL03Po6XK47MMguw5cXhEwSwWmSPh5DE7chrIzzb/AOc7unXbPB54uXuOXDKlrG5pp2l4b8snilV9vxl0sLSGiUu5GbbuDwBt3NB369KD4RHBrJ+EboeHTubr4iia9uO3XvV7ErpInNOzgN4+5zC5p/3HrstDwTtR6awlDEY3E4Grj6MDK1eFlubZkbGhrR9z9AATRa98c4LNFRUjy7rD8Qwfrc31aeXdYfiGD9bm+rTRa98c4LLuipHl3WH4hg/W5vq08u6w/EMH63N9Wmi1745wWXdFWcNqm4/Iw4/M0oadmxzeLTVZnSwzEAks3LWlr+UF2xBBAOxOx2sy568OrDm1RawiItaCIiAiIgIiICIiAiIgIiICIiCF1s4s0bnnA7EULBB/7NyicIAMNQAAAFePoP7IUrrj+Zef+T7H7tyi8L956H+wj/uhejg/B+v4XsdxERZIIiICIiAiLo4zOY/NPutoXYLrqVh1SyIJA/sZmhpdG7bucA5u47xug7yIiCD1AdszpQjv8rN6/wDYTK/qgah+/OlPlZv7iZX9asp2UeH5lZ2QIiLhQREQEREBERAREQEREBERAREQQmuP5l5/5Psfu3KLwv3nof7CP+6FKa4/mXn/AJPsfu3KLwv3nof7CP8Auhejg/B+v4XsfnOy3YcJkJMbEybIsryOrRyHZr5Q08gPxF2y8k8I9RZjMaj0Jm8bqDV+qb9fEX7uscfk7FltSrfbBysibGQ2Nju2dIxsTQRyt5uXdocvYFmLt60sezXc7C3Z43adx6fiXnPhV4PWr9Fa3wORfaxWncPi+0bPVweayttmQjMbmMiNe090cLGktcOXmI5AAQsaom8IrHB+nxb15i9F69qZUP8AKliC9kJp9VyzVJqrn/Z4G47xQRxOa3ma3lfzNc0bvd134cBkdQUOGeluIDtX6it5d+tvJktWzkpH1JKb8vJUMBhPmnzDuHuBeCBs4AADfcJwD0FpzVTdRYzANpZNk77UfZWpxXjmeC18jK/P2THEOcCWsB6lSMfCTScWlqunG4rbDVcgMrDW8Zl820LJsiTm5+Y/ZiXcpPL6NtuikUyPOVY8V+L+S1vmdPX5KN7GZ67icc72VS069DxeTljbNQbUfHNuAHu7R5Lg/oWDba/6MxOW1fx64iHO6lzUVPAPw8tfE4/JzQU2TOqNklJa0gvYXN6sPmu3cXNJPS+5vgHoLUOq5NSXcA12XmkjlnlhtTwx2Hx7cjpYmPEcrhsNi9pPQK0YvSOJwufzebp1OxyeadC6/P2j3dsYmdnH5pJa3ZvTzQN/TuVYpntHlzSWsdRDiHw91biLepPYZq7NT0W+yDUBteOwPhnex7aXZ8tZoMQLC1/NsAHN85cmk2S8KtCeEBrbC28tczOGzWXir17eTsWKwIige2V8DnljntJ3LyOYtG2+y23H+Dhw6xWTqZCppwQ2qdxt+m5tyxy1Jg/n3gb2nLE0u+2YwNa4dHAjopmPhBpGLWGQ1QzDtbmMiwsuOFiXsLILOzJkg5uyeSzzS4sJI9KmbIyXhJo/idR1VpjNzZN0+nbUD5Ms67q2bLtvMfCTHJDC6pG2FwkLHfY3BvKSNj0XotUPQ/AzRHDjLHJadwpx9vsnwMJuTyshjc4OcyKOR7mxNJa07MAHQK+LOmLQIPUP350p8rN/cTK/qgah+/OlPlZv7iZX9YZT7tHh+ZWdkCIi4UEREBERAREQEREBERAREQEREEJrj+Zef+T7H7tyi8L956H+wj/uhWm5Ujv0560w5oZo3RvHutI2P7Csw1FqW3wrwkZzGPlyFKAsrV7dKWMy2nbbMY2Fzg98xA25Iw8uPcPQPQwJirDmi8RN765t92W2FyRQFfO5y1XimbovMNbI0PAknqMcARv1aZwQfiI3C5PK2e+BmV9apfXrfmfNH8o6lk2ihPK2e+BmV9apfXp5Wz3wMyvrVL69Mz5o/lHUsm0Wf8QuL0PCrTjs9qrA5DEYps0cHjEk9V+8jzs1oa2Ykk/EO4E9wKscWbzc8TJI9H5OSN4DmvbbpEOB7iD2/UJmfNH8o6lk6ihPK2e+BmV9apfXp5Wz3wMyvrVL69Mz5o/lHUsm0WIcSvC40jwf1TDp7WFDK4PKzQNsxxzRRvY+NxIDhIx5Ztu0jv6bK8aN4ns4h4ZuW0zhrGcxxeYzYo5CjI1rwAS121jzXbOadjsdnA+kJmfNH8o6lk3qH786U+Vm/uJlf1TcXh8nmsvSvZOl5LqUJHTQ1nzNklllLHMDn8hLWta17thu4lxB83kHNclyZTVE5tMTe0flJERFxoIiICIiAiIgIiICIiAiIgIurk8pTwuPsXr9mKnTrsMks87wxjGj0knuVGEec4pedIbmmNHPB2gLZK2UyTT6XHdr6kRHXl2EztxuYeUteHczGv7ORydrB6OpxZrMV3dlbuWHOZjse70iWVoPaSgHfsI93dweYg4PXa01w6q4jLHO5S1LqLU7muZ5WvMaHQMd3xV42jlgj6AbN853K0yOkcOZWHD4ahp7F1sbi6UGPx9Zgjgq1YxHHG0dwa0dAF3EBERAREQeSPDt8HjiT4QTMBV0zlMFT0zjR2s1XI3JYJZrT38nN0jcwtawt23cDu54AJ2323wdtI6y0Bwlwmmtc3MbkczimGoy3jJpJY5a7fuXMXxsIcG+bttts0HfqVaNfUhkdNurGhVyfaWqm1a5P2MbiLEZ5ub/AEm7czR/Sc0D0qxICIiDzV4Y/giyeE2NJT43JVMLksZaMVq3ZY5xdSfsX8oaPOe0gFrSWg8zt3BX3hNg6PATA4ThxJAyphoHPgwuUa5xjuFzzIWTFxPZ2S57yQDySdXRhu5ij1hdLMYajqDF2cdkqsV2jZYY5q8zeZr2/GP/ABsg7qKj0Mnb4fX6WGzdyS9g7crKuLzNlxdNHK7cMq2nHvcdg2Kcn7ISI3/ZeR1i8ICIiAiIgIiICIiAiIgIiIC6eYy9LAYm7lMlZjpY+lA+zZszO5WRRMaXPe4+gAAkn4l3FQOKG+UzmhdPyc3iOSzPa22gHaSOtBLYYw7HuMsUJO+4IaWkEOQcuHwdvW2Rg1BqSq+vTglbPiMHYG3i5H2tiw3uM5PVrTuIhy7efzFXpEQEREBERARFA3shNnZJcdip5YYXxzxzZmm+J3icrXBnI0PDg6Xcv6FrmtMZDuuzSHWsivq7UFeJjMdkcXiJ3STvMjnyw32cvZta0Dl81r3uJJJB5PN36izrirVoqcDIYWBkbe4d/wAZJPpJPUk95XKgIiICIiDo5zCUdSYe5islXbaoXInQzQuJAc0jY9R1B9wjYg9R1VY4W5u7axmQwOYsOt53TlryZbsybc9tgY2SCydgBvLC+NzuUcok7Ro+1V1WeW+XA8d6ErXObHqPBS15Bt5pmpzNfF6ftiy3P6OojHuBBoaIiAiIgIiICIuOxZiqQSTzyshhjaXPkkcGtaB3kk9wTaORFV3cUdHtOx1RiPd/lsfUe73r57aWjvhTiPXY/nXRo+NwTyllmzuWlFVvbS0d8KcR67H86e2lo74U4j12P500fG4J5SZs7lpWccbLtfTtDSuqbU8dSrgc/VmsWJnhjIobHPSkc9xIAa1tvmJPQBu/oU97aWjvhTiPXY/nXkzw+OEmkuOWlIdV6b1Bi5ta4KuY2V470ZN+qCXmEDm+3aXOc3bv5nDqS3Zo+NwTykzZ3PYml9Y4DW+Pff05nMbn6LJTA61i7cdmJsgAJYXMJAcA4Hbv6j3VMLzd4HculuE3g76Twl/OYqhlpIXXr0EltjXsmlcXlrwTuHNaWtIPdyraPbS0d8KcR67H86aPjcE8pM2dy0oqt7aWjvhTiPXY/nT20tHfCnEeux/Omj43BPKTNnctK45546sEk00jIYY2l75JHBrWtA3JJPcAPSqlf4waLx1V08mpcdK0Oa0MgsNkeS5waNmtJPeRue4DckgAkRUOvNN5e0LOZ1VhoYoZLMUWOq5Rj688LtmsfPuG87+QOPJ9o0ykeeWNkTR8bgnlJmzuWNly1q0DxGabH4c+K2oMpWkjc6/Gfsj2MBDuRhHZtLzs4h7w3kIa9TtSnBQrtgrQsghaSQyNoaASSSdh6SSSfjKrntpaO+FOI9dj+dTGH1FitRRSSYvJVMiyMgPdVmbJyk9RvsTtv8axqwcSiL1UzEeCWmEiiItKCIiAiIgLPeKJNPVXDPIh/IIdROryd/nsmo24+X9IYj1/0VoSzzja4QYPTdotBMGqMMAT6O0vRQ7/AP5UGhoiICIiAiIgKl6tIyGr8LjZwJKbati8YXDdr5WPhbG5w9PLzuIBB6kHoWhXRUnUP4R8R8k2/wB9WXXkvxPpP2lYSaIi6EEREBERAREQEREBERAUFqAtxuTwmThHZ2xfgqOkaOr4pXhjmO90dQdjvsWg94U6oDWH3HDfLFH+IYtuFrriN7Kna0JEReOxEREBERAWe8dy5mgq0jHcpj1BgpN99ujctUcf2BaEs849hvtbSlxIDcriXbgb9RkaxH7Qg0NERAREQEREBUnUP4R8R8k2/wB9WV2VJ1D+EfEfJNv99WXXkvxPpP2lYSaoXEvihPonJ6fwWHwcmpdUZ583iOObYbWjEcLQ6aWWZwIYxoc0dGuJLgACr6sz4qcPtRZnVOldYaPuY6HUWAbar+J5jtBUuVrAYJGOfGC5jgY2Oa4A9QQQQVum9tSKRxA19r2lrnhI5mm5qmYvT5aKzpqvm2+LWOSv5j5Jg0Ncxv3QbsLh6G83Rd+/4VGPwmhpMnl8PHiNRx52XTkmGu5SGKBl2NvO4utv2YIezIf2hH9IANJIBnjoTWeota8O9S6hkwUNnT82SfcgxkkxYWTwdnE2Mvbu8j+kXcg9wehVLK+DxqKSxl81jclioNRw60n1Rh/GRJLWkhkqxV317I5QW8zWv3LObbzSCeoGH/LsHJhfC8w1/B5+SbGwW8/jLFKpDjdP5aDJw5Ca25zK7ILDOVu5c1wcHBpbtvsdxu4ta61xDwzkuZnCHQtuLPYaOOfHZsWRNDJeibK0vYyMtHKS1wI2IcRuRuprUvDHWfELQphzM2nMBqrH5etmcNJh2TT1YZa7mujbOXhjpA49oDs1uweOhI69TXvDbiBxh4d39O6vi0nA2e/jZm18dPZfE+GGyySwJHvjB3exuzWho2O+7uu4a7C9cMOJJ4o1srlqGN7HTEdp1bFZR0/M7JtYS2SZsfKOSPnBawlxLwCdmjbfucTeIVHhfo+1nr1ee7yPir16VQAzWrEr2xxRMBIG7nuaOvd1PoVKwOLi8H/MZ9h7T2A5WyLmMx+Kxtu5PjrTwTYibFXheGwOIEjTuA1znjbYhcWu7WJ4/wCm5dOYG1l8Rnak8GXx97J6dv1oIbFeZkkZcZ4Y2uBcAC0O32JIHRZX1d47mT4x6i0Zo/KZvWehvI0sUlavjqOOy0d6TIWJ5OzjgB5GcjucsBJ3bs7cE7FdCTwi3aRdqKtxB00/SuRxGIGcjgp3m5CO5WMnZbRvDGfZBKWMLCB1kbsSDumpdAcReJ2jL2O1RPpjDZSrYp5HCz4Z1iwyO5XmEofN2jWeY4ta3laNwC7zndNoDU3g+6q4uy6myeuMliMTlbeDbhMVDgDLPBU2sMs+MSOlawvcZYYfNAADWEbkndSc7sHXPFbV0fGnSljWODm0LgY9OZfJT1G5gW45mR+Lu55mMa0CSJvN02dtznlceq7GifDBxOq9Uadx81HFVqGobArY99LUdW7eje5pdGLVSPzoeYDY7OfyuIDtt12rHCHX3EbVmPu8QJNMw42LT+UwVlmAnsOll8bbE0yASxgDpGfN3833Xb9LBwm0dxF0c3C4PPO0lewGJr+KtylKOcZC2xjOWJzoy0Mid0aXEOfv1223UjOuNcUBrD7jhvlij/EMU+oDWH3HDfLFH+IYurC9+GVO2GhIiLx2IiIgIiICzvj9t7WVnf3yxf8AzCutEWd8fvwY2flLF/8AMK6DREREBERAREQFSdQ/hHxHyTb/AH1ZXZUvVwbjtW4XKWCIqRq2KLp3HZscr3wujDj3Dm5HAEkDm5W9S4BdeS/Et3T9pWEiiAggEHcH0ouhBERAREQEREBERAREQFAaw+44b5Yo/wAQxT6gs9y5PKYXGQHtbYvwW3xs6mOKJ4e57vcHQAb7blwAW3C1VxO5lTtaAiIvHYiIiAiIgLO+P34MrHyni/8AmFdaIs849hzuG0wYCXHKYr7Xv28o1t/2INDREQEREBERAXHPBHahfDNGyWKRpa+N7Q5rge8EHvC5EQVh/C7R0juZ2lMK4+6aEX0V+far0Z8E8J+r4voq0oujSMbjnnK3neq3tV6M+CeE/V8X0U9qvRnwTwn6vi+irSiaRjcc85LzvVb2q9GfBPCfq+L6Kg7XCnSlzV9KM6KpR06VZ9jxuOCJleSV5LBE6MDeQhvM7r0bu09TttoqrmkKLHSZfMSYqLG38nceZXR2vGDYiiJhgk5h0bzRMY/kb0bznfzuYlpGNxzzkvO9w+1Xoz4J4T9XxfRT2q9GfBPCfq+L6KtKJpGNxzzkvO9Vvar0Z8E8J+r4vor4eFWi3Ag6Twux/wBQi+irUiaRjcc85LzvZvBwz0jpiSGnb0thJMQ51epQsOpGadriwtLbD3hxO5a3aVzt3OlDSNwHPn/ar0Z8E8J+r4voq0EbhVuJ40PFBWeGs0zBDDWgkJklmgkMvIGvPUmPZ8YDj9oGuLzt1DSMbjnnJed7j9qvRnwTwn6vi+ipnD6fxenoXxYvG1MbE8guZUgbEHEd24aBupBFjVjYlcWqqmY8S8iIi0oIiICIiAs849MMnDvlG27szhh1IH/tOr7q0NZ5x2aJNC0mF4Zz6jwDd3b9d8xTG3T3e7/f6EGhoiICIiAiIgIiICIvjjs0oPqKo+Wbn9e78wTyzc/r3fmCCY1Xdt4/Tt6bHww2ch2fZ1YbFjxeOSZxDY2uk727ucBuOvXpudl2cJhqWncNQxONrR08dQrx1a1eIbMiiY0NYxo9wAAD8iqGUecx4rHdbHaZBOy1G2VoPJJGeZjwPda7Yg+ggFfq1rBtK/So2MrBXu3S8Va0sjGyWCxvM8MaeruVvU7b7DqUF6RRWBtzW45jM8vLSNt1KoCIiAvhAcCCNwe8FfUQVqaduhoZZ55WM0zEyxbtXblt7n0SXh/9IEdgA6UklwEQY0AFn3Kyoq/NI/Sck08j3zYWWSW1as27bGMxjBGD05gPsO7ZHElxLC8ADk6RhYEXxrg9oc0hzSNwQdwQvqAiIgIiICzzjgGyaYwUTiQJNUYLbYb7luTrvH9xaGs74zbSQ6Lr9/bapx+w/sPMv/8ANBoiIiAiIgIiICIiAvjvtT+RfV8d9qfyIPO/hA8RMvw90niRga8k+azmXr4aq6KKOV8TpQ9xe1kj2Mc4NjcGhz2t5i3c7dDlmf4icWtE6B1vfvRZSKvSpVrGMzOoKWPjsNsGyxkkLoqsr43sLHAh3K0jzh7hW+8QNAYjiXpyTC5lk3YGWOxDPVlMU9aZjuaOWJ46te0jcH8+4JCrNngZTymis7prMap1Nnq+Y7ITW8jcjfNEI3hzREBEI2dR18zc+k9yCi6k4m6n4M6o1TWzeZOsqtXSE+o6/bU4az4p4pmxuhHZNH2JxkYfO5nN2PnFR0WG1hS4zcFr+q9Ws1DPeGTmNSLHxV4akholxbE5nnOZ1288uJ2B3G+y2PN8LMHqjVdvOZNklt9jDT6fmplzfF5asr2PfzDbm592Abh22x7t+qrmlfB5xml9Rabyx1RqfMO06JmYyplL0c0MDJIjEWbCIOIDT0JcSNh1I6IN10x9yn/tBTahNMfcp/7QU2gIiICIiAiIggQH6YsxRxxT2MTYlZFHFBDE2PGNEZG52LT2RLWjoHOa6QkkR/c55cc8EdqGSGaNssMjSx8b2hzXNI2IIPeCFAU5hpW9XxcxijxVhza+KZWqyAV+WPrDI4btA808jjyDqGbbgFwWNERAREQFnvFEPn1VwvqsbzNl1K58h5d+VseNvSA/F5zWDf8A6QWhLOtaf5Xxi4b1O8wR5TI7f2IY4N//ANrb/eg0VERAREQEREBERATvREHV8l1PxeP8yeS6n4vH+ZdpEFb0vD4/XyD7kONc6O/YhjNCTtG9m2QtaHn0SbDzm+g7hTXkup+Lx/mUPoiJ0NHJh0OKhLsrccBiTuxwM7iHSf8AXHvk/wCnzKxIOKCtFWBEUbWA9/KFyoiAiIgIiICIiAuC9TZkKVirI6Vkc8bonOgldFIA4bEte0hzT16OaQQeoIK50QQuCuzxSSYq+XeN1QGwzz2Iny3omtYDYLWBvKeZ2zhyNAdvtuCCZpRWfw8mRhbYpeKQZmsHeJ3bVUTiEnbmG24dyuA5Xcrmkj09AufD5aHNUzPEyWMslkgkjnidG9j2OLHDZwB23BIdts5pa5pLXAkO8iIgLPJh4/4QdM7A+StLz+nq3xq3F6Pj8S/YtDWeaNab/GHiLkehFaLGYYEHfbsopLJHxfy4H/ePiQaGiIgIiICIiAiIgIiICIqPxuh1fNwo1L7Ash5M1fFVM+OnEEc5L2ODzGGSNc0l7WuYNwdi/fptugldDxCGjkw2PFR82VuOIxLy5hJncd5PcmP+cHofzKxrwV/5Pjibxg4w6xz17UeowdG4oyPtVIsTSri3enLjsXRwtduDzyOIO+/Lv0cveqAiIgIiICIiAiIgIihNWZyfC0a7acbJchdnFWsJQTG15a5xc/bqQ1rHu2G2+wG433GdFE11RTAm15S8MTwuYfBvuQxYDH3cprO9A2EQZKKw3EQwtdziY78rZZDzOYOxcPT2jvsbGncX4vPSkOOsMnE70thq0w3v9AdA4/tKhNXcNBr3CTYjUWeuZrGzDz612jQkbv7o3rdD7hGxHoXXovzx59Ft3vx4KfFHIcY+AektUZiaKfM2YHw3pImBgfNHI6MvLR0BcGhxAAALugA6DWli/DXgtV4Qaa9j+kdQ5jEYgTvsCqBWmAe/bmIMkLiAdu4Hb4lavI+d+GmY9Wo/4dNF/cj+3Qt3r8s94MEXsdqrMg83lXUuReHFoBc2CXxNp+MctVux9zZcnkfO/DTMerUf8Oo7T2hrulcRBi8Xq3MVqMJcWR9jTeQXOL3EudASSXOJ6n0pov7kf26Fu9p6Kg+R878NMx6tR/w6eR878NMx6tR/w6aL+5H9uhbvX5FQRiM6Dv7NMufiNajt/DqX0vm7rsjYw2TlZZtwwtsRW2M5O3jLi3zmjoHtI67dDuDsN9hhXk800zVFUTbdf8xBZZ0RFyIKva01tQ0TQZNa5p7U5LK1OL7pM4d/5GjcbuPQbj0kA2AkAEk7AekrzHmNQSauzdvNyucWWXctZrht2dcdI2j8o88/G8/EvX9G5FGWYk5/u07fxC96ay3EzVealLm5FuFhPdXx8bHkdfTJI0l35QG/kUV7JNRn/wB58r+lb9FdFF9xTkuBRFqcOOUMc6Xd9kmo/hPlf0rfop7JNR/CfK/pW/RXSRZ+oweCOUGdLp6Wx82iKtutgMlcxNe3bkvTxVSxjZJ5Du95Ab3nYfmA9CmfZJqP4T5X9K36K6ShctqynhtR4HCzxzutZl07a742gsaYo+0dzkkEbgdNgevuLGcLApi80RygzpWf2Saj+E+V/St+iuaDWGqajg6HU1/mHombFK0/lDmf8NlGIrOT4M6pojlHQzpanonjDJbtxY7UcUFeWUhkORr7the49zXtJJjce4HctJ/0SQDqa8ryRsmjdHI0PY4FrmuG4IPeCFtfBvU0+d01NTuSumu4ubxZ0rzu6WMtDo3n3TynlJPUljj6V8r6V9HUYNPr8GLR2x+V2r6iIvmAREQFTuIH310X8sP/AIC2riqdxA++ui/lh/8AAW11ZL8T6T9pWHeREXSgiKtcQNf4/hzg4slfgt3HWLUVKrSoRCSxasSO5WRRtJA3PU9SAACSRspsFlRU/RnEynrLUOawbcXksTlMRWp2bdfIsiaWCy17mM3ZI8FzRG7m9AO2xKuCbQRFDaP1hiteaer5vC2Daxth8rIpjG5nMY5HRu6OAI85ju8IJlRWL/CWPkh375qlVFYv8JY+SHfvmrZHu1+ErC8IiLyUdPMRST4i9FDv2z4HtZt/pFp2/avLOFLXYeiW7cvYM22O/wDRC9ZLztrnSUui9QzRCMjFXZXTUpQPNaXbudCT6HNPMQPS3u35XbfU+g8ammqvCnbNpj6XOxBIoXUWDyOZNc0NRX8D2fNzilDXk7Xfbbm7aJ+22x7tu8779Nof2E6g5SPbCzm+/f4nj/8ADL6ua5ibRTM8urBXfCNv3aeisZHBbjoULeYq1clamDzFFVcXcxk5HMcGF/Ztds5vRxG/VZtqrRXsV4c66noahwstJ9Sox+K07FJDFXl8YYWz7Onl5HObuPN5d+UHvG63rEaUuVhZjy+obupak8ZidUyVaqI9j3naKFhO46bEkde5dmpofTlDFT4ytgMXXxtgh01OKlG2GQg7guYG7HqAeoXFiZNONVNc6rxbX2apjsmysY1pQdwy1ZqeLRlY4+SbRdq92Fckh9mKZjWzbHfmkDXu847k9N91wad0/o3E8ROE9vTE9e1ZuQ3ZLVtloyzWf8kJ7SXdx3cXE9T3EkdO5b+7GU3ZFuQNSA32xGuLRjHaiMkOLObbflJAO3duAoVnD3T9F00+Kw+OwuScJDHkaNCBs8L3tLTI0lhHNsT3gg9xBHRSrJZzrxa14nwtMTq8RY0VNGiNQA/hCzh+I08f/hl+odF5+KaN79f5uZjXAmN1THgOG/cdqwPX4iuzPq4J8uqLgtE4Cxv8saplAPZFlOPf0c47Yn9j2fsWcucQWNZG+aWRwZHDE3mfI49zWj0krfuGukX6O00yvY5TkLMjrVtzO7tHADlHuhrWtbv6eXf0ryfTGNTh5NOHO2q3lN/wzhakRF8GCIiAqdxA++ui/lh/8BbVxVO4gffXRfyw/wDgLa6sl+J9J+0rDvLFvCzx7Mtw1w9GSSWKOzqfDQukgkMcjQ67ECWuHVpG/QjqCtpXTymGx+bgjhyNGtkIYpo7DI7ULZGslY4OY8BwOzmuAIPeCAQuiYvFkeWuMWhsdQ4q6N0HUZpvTOiLGLuXq9LN05ZMbdyQljDg9kc8PPKIzzN53OHV52JIIi9V8H8OzQvDSlmcjhteUJNfxQUZqkLnValObmbNSiL5ZXGMSQkEF522Df6K9Y6j0rhNYY/xDPYehm6PMH+K5GqyxFzDuPK8Eb/GvyzSGCjx+OoswuObRx0zbFKs2pGI6srd+V8TdtmOG52LdiNysMwecNZZm/o7VnhEs0y4VdQQ6Vxc+LrwkCUMjr2WmSJne7k3HcDseUelQ3Djh8/EvxWq9Faz0jPkxhrd5uOwFWw23mwa5DfGe1uy9oWzOicXFnMHDbcbkL1hJgsbNmYsvJjqj8rDEa8d50DTOyMncsEm3MGk9SAdlHYLh/pfS+Rs5DDabxGIv2t+3tUKMUEs253PO5rQXdevVM3WMP8AB20jwtzOjdIZ91jH5nW+ZxxGQuXb5kvXLMkB8cilY5+7uXeUGMghob0A23Xc8C3T+lcHwpgfiKWMpahmntw5QVmsbZd2VydrGyged5rSAN+4Ee6tko8PtLYzUE2dp6axFTOTEmXJwUImWZCe8ulDeY7+ncrnxujsBhszey+PweNo5W//ACu9WqRxz2Ou/wBkkaA5/Xr1JVimwmFFYv8ACWPkh375qlVFYv8ACWPkh375q3R7tfhKwvCIi8lBdLL4eln8fLRyNaO3UlA5opBuNwdwR7hBAII6ggEdQu6isTNM3idYx7L8C7kMrnYXNMdAfta+TiLnN/JK0gkejq0n3SfTEngzq/0TYQ/94m+qW7ovZo9L5XTFpqifGFuwf2mtYf1uD9Ym+qT2mtYf1uD9Ym+qW8Is/wBZyru5Grcwf2mtYf1uD9Ym+qT2mtYf1uD9Ym+qW8In6zlXdyNW5g/tNaw/rcH6xN9UuetwT1PM8NsZLE02HvfE2Wcj8gPJv+dbiik+mcrnZMcjVuU/RnDHGaPlFvnkyWU2LTds7bsB7xG0dGD8nU+klXBEXkYuLXjVZ+JN5QREWoEREBU/X7Scno09Nm5d5JJ/1G2P+JCuCjs9g4c/jzWlfJC9r2yw2ISBJDI07te3fcbj3CCCCQQQSDvwa4oriZ2a/OLLCNRRr9P6ta7ZmVw0jR3Ofj5Wk/lAm/8AHxdy+eQdYe+eD9Rm+uXd7Pjjz6Fu9JoozyDrD3zwfqM31yeQdYe+eD9Rm+uT2fHHn0LJNFGeQdYe+eD9Rm+uTyDrD3zwfqM31yez448+hZJoozyDrD3zwfqM31yeQdYe+eD9Rm+uT2fHHn0LJNReKG/ErcejEO3+LeZu3/A/mX3yBrD3zwg/7jN9cpvTmm3Yd09q3a8fylgNbNZEfZsDW78rI2bnlaNydtySSdyem2NVdFFFVqomZi2q/RdibREXmMRERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREH//Z",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: RE-TRY SOLUTION---\n",
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "---CODE BLOCK CHECK: FAILED---\n",
      "---DECISION: FINISH---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'error': 'yes',\n",
       " 'messages': [('user',\n",
       "   'How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?'),\n",
       "  ('assistant',\n",
       "   'This code demonstrates how to directly pass a string to a runnable in LangChain and use it to construct the input for a prompt. The example uses a simple prompt template that takes a user input string and formats it into a prompt for a language model. \\n Imports: from langchain_core.prompts import PromptTemplate\\nfrom langchain_core.runnables import Runnable \\n Code: # Define a prompt template that takes a user input\\nprompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\\n\\n# Create a runnable that uses the prompt template\\nclass JokeRunnable(Runnable):\\n    def invoke(self, input_string):\\n        # Construct the input for the prompt\\n        input_data = {\\'topic\\': input_string}\\n        # Generate the prompt\\n        return prompt_template.invoke(input_data)\\n\\n# Create an instance of the runnable\\njoke_runnable = JokeRunnable()\\n\\n# Pass a string directly to the runnable\\nresult = joke_runnable.invoke(\"cats\")\\n\\n# Print the result\\nprint(result)'),\n",
       "  ('user',\n",
       "   \"Your solution failed the code execution test: name 'prompt_template' is not defined\"),\n",
       "  ('user',\n",
       "   'Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:'),\n",
       "  ('assistant',\n",
       "   'This code demonstrates how to directly pass a string to a runnable in LangChain and use it to construct the input for a prompt. The example uses a simple prompt template that takes a user input string and formats it into a prompt for a language model. \\n Imports: from langchain_core.prompts import PromptTemplate\\nfrom langchain_core.runnables import Runnable \\n Code: # Define a prompt template that takes a user input\\nprompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\\n\\n# Create a runnable that uses the prompt template\\nclass JokeRunnable(Runnable):\\n    def invoke(self, input_string):\\n        # Construct the input for the prompt\\n        input_data = {\\'topic\\': input_string}\\n        # Generate the prompt\\n        return prompt_template.invoke(input_data)\\n\\n# Create an instance of the runnable\\njoke_runnable = JokeRunnable()\\n\\n# Pass a string directly to the runnable\\nresult = joke_runnable.invoke(\"cats\")\\n\\n# Print the result\\nprint(result)'),\n",
       "  ('user',\n",
       "   \"Your solution failed the code execution test: name 'prompt_template' is not defined\"),\n",
       "  ('user',\n",
       "   'Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:'),\n",
       "  ('assistant',\n",
       "   'This code demonstrates how to directly pass a string to a runnable in LangChain and use it to construct the input for a prompt. The example uses a simple prompt template that takes a user input string and formats it into a prompt for a language model. \\n Imports: from langchain_core.prompts import PromptTemplate\\nfrom langchain_core.runnables import Runnable \\n Code: # Define a prompt template that takes a user input\\nprompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\\n\\n# Create a runnable that uses the prompt template\\nclass JokeRunnable(Runnable):\\n    def invoke(self, input_string):\\n        # Construct the input for the prompt\\n        input_data = {\\'topic\\': input_string}\\n        # Generate the prompt\\n        return prompt_template.invoke(input_data)\\n\\n# Create an instance of the runnable\\njoke_runnable = JokeRunnable()\\n\\n# Pass a string directly to the runnable\\nresult = joke_runnable.invoke(\"cats\")\\n\\n# Print the result\\nprint(result)'),\n",
       "  ('user',\n",
       "   \"Your solution failed the code execution test: name 'prompt_template' is not defined\"),\n",
       "  ('user',\n",
       "   'Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:'),\n",
       "  ('assistant',\n",
       "   'This code demonstrates how to directly pass a string to a runnable in LangChain and use it to construct the input for a prompt. The example uses a simple prompt template that takes a user input string and formats it into a prompt for a language model. \\n Imports: from langchain_core.prompts import PromptTemplate\\nfrom langchain_core.runnables import Runnable \\n Code: # Define a prompt template that takes a user input\\nprompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\\n\\n# Create a runnable that uses the prompt template\\nclass JokeRunnable(Runnable):\\n    def invoke(self, input_string):\\n        # Construct the input for the prompt\\n        input_data = {\\'topic\\': input_string}\\n        # Generate the prompt\\n        return prompt_template.invoke(input_data)\\n\\n# Create an instance of the runnable\\njoke_runnable = JokeRunnable()\\n\\n# Pass a string directly to the runnable\\nresult = joke_runnable.invoke(\"cats\")\\n\\n# Print the result\\nprint(result)'),\n",
       "  ('user',\n",
       "   \"Your solution failed the code execution test: name 'prompt_template' is not defined\"),\n",
       "  ('user',\n",
       "   'Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:'),\n",
       "  ('assistant',\n",
       "   'This code demonstrates how to directly pass a string to a runnable in LangChain and use it to construct the input for a prompt. The example uses a simple prompt template that takes a user input string and formats it into a prompt for a language model. \\n Imports: from langchain_core.prompts import PromptTemplate\\nfrom langchain_core.runnables import Runnable \\n Code: # Define a prompt template that takes a user input\\nprompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\\n\\n# Create a runnable that uses the prompt template\\nclass JokeRunnable(Runnable):\\n    def invoke(self, input_string):\\n        # Construct the input for the prompt\\n        input_data = {\\'topic\\': input_string}\\n        # Generate the prompt\\n        return prompt_template.invoke(input_data)\\n\\n# Create an instance of the runnable\\njoke_runnable = JokeRunnable()\\n\\n# Pass a string directly to the runnable\\nresult = joke_runnable.invoke(\"cats\")\\n\\n# Print the result\\nprint(result)'),\n",
       "  ('user',\n",
       "   \"Your solution failed the code execution test: name 'prompt_template' is not defined\")],\n",
       " 'generation': code(prefix='This code demonstrates how to directly pass a string to a runnable in LangChain and use it to construct the input for a prompt. The example uses a simple prompt template that takes a user input string and formats it into a prompt for a language model.', imports='from langchain_core.prompts import PromptTemplate\\nfrom langchain_core.runnables import Runnable', code='# Define a prompt template that takes a user input\\nprompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\\n\\n# Create a runnable that uses the prompt template\\nclass JokeRunnable(Runnable):\\n    def invoke(self, input_string):\\n        # Construct the input for the prompt\\n        input_data = {\\'topic\\': input_string}\\n        # Generate the prompt\\n        return prompt_template.invoke(input_data)\\n\\n# Create an instance of the runnable\\njoke_runnable = JokeRunnable()\\n\\n# Pass a string directly to the runnable\\nresult = joke_runnable.invoke(\"cats\")\\n\\n# Print the result\\nprint(result)', description='Schema for code solutions to questions about LCEL.'),\n",
       " 'iterations': 5}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?\"\n",
    "app.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATING CODE SOLUTION---\n",
      "---CHECKING CODE---\n",
      "Why don't cats play poker in the jungle?\n",
      "\n",
      "Because there are too many cheetahs!\n",
      "---NO CODE TEST FAILURES---\n",
      "---DECISION: FINISH---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'error': 'no',\n",
       " 'messages': [('user',\n",
       "   'How can I make the output of my LCEL chain a string?\\t?'),\n",
       "  ('assistant',\n",
       "   'To make the output of your LCEL chain a string, you can use an output parser that converts the final output into a string format. In this example, we will use the `StrOutputParser` from LangChain to achieve this. The `StrOutputParser` takes the output from the chain and formats it as a string, ensuring that the final result is in the desired format. \\n Imports: from langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_openai import ChatOpenAI \\n Code: # Define a prompt template\\nprompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\\n\\n# Initialize the chat model\\nmodel = ChatOpenAI(model=\"gpt-4\")\\n\\n# Create a chain with the prompt and model\\nchain = prompt | model | StrOutputParser()\\n\\n# Invoke the chain with a specific topic\\nresult = chain.invoke({\"topic\": \"cats\"})\\n\\n# Print the output as a string\\nprint(result)')],\n",
       " 'generation': code(prefix='To make the output of your LCEL chain a string, you can use an output parser that converts the final output into a string format. In this example, we will use the `StrOutputParser` from LangChain to achieve this. The `StrOutputParser` takes the output from the chain and formats it as a string, ensuring that the final result is in the desired format.', imports='from langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_openai import ChatOpenAI', code='# Define a prompt template\\nprompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\\n\\n# Initialize the chat model\\nmodel = ChatOpenAI(model=\"gpt-4\")\\n\\n# Create a chain with the prompt and model\\nchain = prompt | model | StrOutputParser()\\n\\n# Invoke the chain with a specific topic\\nresult = chain.invoke({\"topic\": \"cats\"})\\n\\n# Print the output as a string\\nprint(result)', description='This code sets up an LCEL chain that outputs a string by using the StrOutputParser.'),\n",
       " 'iterations': 1}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How can I make the output of my LCEL chain a string?\t?\"\n",
    "app.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
